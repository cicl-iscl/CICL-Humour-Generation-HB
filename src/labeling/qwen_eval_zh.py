import pandas as pd
import torch
import os
from tqdm import tqdm
from transformers import AutoModelForCausalLM
from accelerate import Accelerator
from torch.utils.data import DataLoader

# Import shared utilities
from eval_utils import parse_args, ListDataset, get_rating, collate_fn

# --- 1. PROMPT & MODEL CONFIG ---

PROMPT_ZH = """
è§‚å¯Ÿç±»ç¬‘è¯æ˜¯é€šè¿‡å–œå‰§è§†è§’å®¡è§†æ—¥å¸¸äº‹ç‰©æˆ–æƒ…å¢ƒã€‚å®ƒä»¬æ¶µç›–å‡ ä¹äººäººç†Ÿæ‚‰çš„ä¸»é¢˜ï¼Œç”šè‡³æ¶‰åŠç”Ÿæ´»çš„æœ€çç¢ç»†èŠ‚ã€‚
è€Œè½¶äº‹ç±»å¹½é»˜åˆ™æºè‡ªå–œå‰§æ¼”å‘˜çš„ä¸ªäººç»å†ï¼Œå› è§‚ä¼—èƒ½äº§ç”Ÿå…±é¸£è€Œå¹¿å—æ¬¢è¿ã€‚
ä½ æ—¢æ¬£èµè§‚å¯Ÿç±»ä¸è½¶äº‹ç±»å¹½é»˜ï¼Œä¹Ÿé’Ÿçˆ±å†·ç¬‘è¯å’Œåè®½ã€‚
ä½ æ‡‚å¾—æ¬£èµå¦™è¶£æ¨ªç”Ÿçš„ç¬‘è¯ï¼Œä½†è¦è®©ä½ å‘ç¬‘ä¹Ÿç»éæ˜“äº‹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æŒ‰0è‡³10åˆ†è¯„åˆ†ï¼š0åˆ†ä»£è¡¨æ¯«æ— è¶£å‘³ï¼Œ10åˆ†ä»£è¡¨çˆ†ç¬‘è‡³æã€‚å¹³åº¸ç¬‘è¯é€šå¸¸å¾—5åˆ†ã€‚
9åˆ†æˆ–10åˆ†æä¸ºç½•è§ï¼Œä»…æˆäºˆé¡¶å°–ç¬‘è¯ã€‚å› æ­¤8åˆ†å·²æ˜¯ç›¸å½“é«˜çš„è¯„ä»·ï¼Œè¯·å‹¿è½»æ˜“ç»™äºˆã€‚
è¯·ä»…è¿”å›åŒ…å«æœ‰æ•ˆJSONçš„å­—æ®µï¼š`rating`ï¼ˆæ•´æ•°å½¢å¼çš„è¯„åˆ†ï¼‰å’Œ`reason`ï¼ˆè¯„åˆ†ç†ç”±ï¼‰ã€‚
ç¬‘è¯å†…å®¹å¦‚ä¸‹ï¼š
{}
"""

# The exact prefix for Qwen models
ASSISTANT_PREFIX = "<|im_start|>assistant\n" 

# --- 2. MAIN EXECUTION ---

def main():
    """Main function to run the joke evaluation."""
    args = parse_args()
    
    # 1. Load Data
    print("ğŸ“¥ Loading jokes from zh_jokes.csv...")
    try:
        # NOTE: Ensure zh_jokes.csv is in the current working directory or provide the full path
        zh_joke_df = pd.read_csv("zh_jokes.csv")
    except FileNotFoundError:
        print("Error: zh_jokes.csv not found. Please ensure it's in the correct directory.")
        return

    jokes_combined = zh_joke_df.joke.to_list()
    print(f"Total jokes loaded: {len(jokes_combined)}")
    
    # Pre-format the jokes with the detailed prompt
    eval_prompts = [PROMPT_ZH.format(joke) for joke in jokes_combined]

    # 2. Setup Model, Tokenizer, and Accelerator
    print(f"ğŸ¤– Initializing model: {args.model_name}...")

    accelerator = Accelerator(mixed_precision="bf16")

    # Initialize dataset and get the tokenizer used within it
    eval_dataset = ListDataset(eval_prompts, args.model_name)
    tokenizer = eval_dataset.tokenizer

    # 3. Prepare DataLoader
    data_loader_collate_fn = lambda batch: collate_fn(batch, tokenizer)

    eval_dataloader = DataLoader(
        eval_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        collate_fn=data_loader_collate_fn,
        num_workers=4,
        pin_memory=True,
    )

    # Initialize model - let Accelerate handle device placement
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )

    # Prepare for acceleration - this handles device placement
    model, eval_dataloader = accelerator.prepare(model, eval_dataloader)

    # 4. Inference Loop
    print(f"ğŸš€ Starting accelerated inference with batch size {args.batch_size}...")
    
    evals = []
    model.eval()
    
    # Generation parameters
    generation_kwargs = {
        "max_new_tokens": 150, 
        "do_sample": False,
        "temperature": 0.0,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id
    }

    for batch in tqdm(eval_dataloader, desc="Evaluating Jokes"):
        input_ids = batch["input_ids"].to(accelerator.device)
        attention_mask = batch["attention_mask"].to(accelerator.device)
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                **generation_kwargs
            )
        
        generated_tokens = outputs[:, input_ids.shape[1]:]
        generated_texts = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)
        
        # Qwen models include the full prompt and a special start token in the output.
        # We need to clean this to get the raw JSON response.
        cleaned_texts = [
            text.split(ASSISTANT_PREFIX)[-1].strip() if ASSISTANT_PREFIX in text else text.strip()
            for text in generated_texts
        ]
        
        evals.extend(cleaned_texts)

    # 5. Process and Save Results
    print("ğŸ“ Processing and saving results...")
    
    eval_df = pd.DataFrame()
    eval_df["joke"] = jokes_combined
    eval_df["model_raw_output"] = evals
    eval_df["score"] = [get_rating(e) for e in evals]

    # Save the final dataframe
    eval_df.to_csv(args.output_file, index=False)
    print(f"âœ… Evaluation complete! Results saved to {args.output_file}")


if __name__ == "__main__":
    main()