{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d95ebe-af93-4901-b43f-a2d16a3c4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface-hub trl[vllm] wandb weave accelerate emoji -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ca0d6-50cf-4d3c-885b-acfab5f3f1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086a4b8-f663-4c68-8655-e4a81d03bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83512436-2346-443b-b250-b33dcf34acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VLLM_CONFIGURE_LOGGING=0\n"
     ]
    }
   ],
   "source": [
    "%env VLLM_CONFIGURE_LOGGING=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aefc22-9280-47f1-ad48-f6a30077a717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b840cd-331f-4591-926b-82181cd4dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a350cad8-d868-46d6-9c0d-12281fc6e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\"parquet\", data_files=\"data/rl_df_train.parquet\", split=\"train\")\n",
    "test_dataset = load_dataset(\"parquet\", data_files=\"data/rl_df_test.parquet\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cffa9f5-40c2-4cdc-9cc3-b248d66c910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Generate the funniest possible joke that contains these two words: 'learn', 'flower'. Return only the joke. \"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3508a0-3a79-4f8c-94f7-dfe2388b33b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_single_joke(text):\n",
    "    \"\"\"\n",
    "    Combat joke stacking.\n",
    "    \"\"\"\n",
    "    if text.count(\"?\") > 1:\n",
    "        return False\n",
    "    if text.lower().count(\"why \") > 1:\n",
    "        return False\n",
    "    if \"Q:\" in text.lower() or \"A:\" in text.lower():\n",
    "        return False\n",
    "    if len(text.strip().split('\\n')) > 3:\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ed78711-1c74-4d01-84d2-0fa56fd09d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "recent_structures = deque(maxlen=30)\n",
    "\n",
    "def extract_joke_structure(joke: str) -> str:\n",
    "    joke_lower = joke.lower()\n",
    "    \n",
    "    if re.search(r\"why\\s+(?:did|didn't|do|dont|don't|were|weren't|was|wasn't)\\s+\\w+\", joke_lower):\n",
    "        return \"why-did\"\n",
    "    elif re.search(r\"where\\s+(?:did|do|were|was)\\s+\\w+\", joke_lower):\n",
    "        return \"where-did\"\n",
    "    elif re.search(r\"what\\s+do\\s+you\\s+call\", joke_lower):\n",
    "        return \"what-do-you-call\"\n",
    "    elif re.search(r\"knock\\s+knock\", joke_lower):\n",
    "        return \"knock-knock\"\n",
    "    elif joke.count(\"?\") == 1 and (joke.count(\"!\") >= 1 or joke.count(\".\") >= 1):\n",
    "        return \"qa-punchline\"\n",
    "    elif any(phrase in joke_lower for phrase in [\" is like \", \" is when \"]):\n",
    "        return \"observation\"\n",
    "    else:\n",
    "        return \"one-liner\"\n",
    "\n",
    "def structure_diversity_reward(completions, **kwargs):\n",
    "    global recent_structures\n",
    "    scores = []\n",
    "    freq = {}\n",
    "    \n",
    "    for s in recent_structures:\n",
    "        freq[s] = freq.get(s, 0) + 1\n",
    "\n",
    "    total = max(len(recent_structures), 1)\n",
    "    num_structures = len(freq) if freq else 1\n",
    "    target = 1 / num_structures\n",
    "\n",
    "    for joke in completions:\n",
    "        s = extract_joke_structure(joke)\n",
    "        recent_structures.append(s)\n",
    "        actual = freq.get(s, 0) / total\n",
    "        reward = target - actual\n",
    "        scores.append(reward)\n",
    "        freq[s] = freq.get(s, 0) + 1\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2574215b-e1cb-42eb-a859-62763636b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "scoring_pipe = pipeline(\"text-classification\", model=\"KonradBRG/RoBERTA-Joke-Rater\", trust_remote_code=True)\n",
    "        \n",
    "def roberta_score(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Scores humor using RoBERTA, but only for valid outputs.\n",
    "    Invalid outputs get 0.0 regardless of what the model thinks.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    roberta_labels = scoring_pipe(completions)\n",
    "    for i, completion in enumerate(completions):\n",
    "        roberta_score = 0.0\n",
    "        if is_valid_single_joke(completion):\n",
    "            roberta_label = roberta_labels[i]\n",
    "            roberta_score = roberta_label[\"label\"][-1]\n",
    "            roberta_score = float(roberta_score)\n",
    "        scores.append(roberta_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c04c677-22f1-4c2e-a9ab-08eaf93bafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_pair_prompt_adherence(completions, prompts, **kwargs):\n",
    "    \"\"\"\n",
    "    Enforces the word pair constraint.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    pattern = r\"contains\\s+these\\s+two\\s+words:\\s*'([^']+)'\\s*,\\s*'([^']+)'\"\n",
    "    for i in range(len(completions)):\n",
    "        p = prompts[i]\n",
    "        if \"two words\" not in p:\n",
    "            scores.append(None)\n",
    "            continue\n",
    "        \n",
    "        c = completions[i].lower()\n",
    "        try:\n",
    "            w1, w2 = re.findall(pattern, p, flags=re.IGNORECASE)[0]\n",
    "        except IndexError:\n",
    "            scores.append(None)\n",
    "            continue\n",
    "        \n",
    "        words_in_completion = set(c.split())\n",
    "        w1_lower = w1.lower()\n",
    "        w2_lower = w2.lower()\n",
    "        \n",
    "        w1_found = any(w1_lower in word.lower() for word in words_in_completion)\n",
    "        w2_found = any(w2_lower in word.lower() for word in words_in_completion)\n",
    "        \n",
    "        if w1_found and w2_found:\n",
    "            scores.append(2.0)\n",
    "        elif w1_found or w2_found:\n",
    "            scores.append(-1.0)\n",
    "        else:\n",
    "            scores.append(-2.0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def headline_adherence(completions, prompts, **kwargs):\n",
    "    scores = []\n",
    "    for i, completion in enumerate(completions):\n",
    "        \n",
    "        if \"two words\" in prompts[i]: # it is a word pair task sample\n",
    "            scores.append(None)\n",
    "            continue\n",
    "        \n",
    "        if len(completion.split()) <= 25: # roughly the max length of the task\n",
    "            if \"headline\" in completion:\n",
    "                scores.append(-1.0)\n",
    "            else:\n",
    "                scores.append(1.0)\n",
    "        else:\n",
    "            scores.append(-1.0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2744f1df-99f4-453a-810a-44075fe6363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def contains_emoji(text):\n",
    "    return any(char in emoji.EMOJI_DATA for char in text)\n",
    "\n",
    "def formatting(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Validates output formatting and penalizes hacking patterns.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        if (\"#\" in completion\n",
    "            or \"How about: \" in completion\n",
    "            or \"This joke\" in completion\n",
    "            or \"Let me know\" in completion\n",
    "            or \"Note: \" in completion\n",
    "            or \"   \" in completion\n",
    "        ):\n",
    "            scores.append(-1.0)\n",
    "        elif contains_emoji(completion):\n",
    "            scores.append(-1.0)\n",
    "        elif not is_valid_single_joke(completion):\n",
    "            scores.append(-1.0)\n",
    "        else:\n",
    "            scores.append(1.0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def length_penalty(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Penalizes longer outputs to discourage joke stacking.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    optimal_length = 16\n",
    "    max_allowed = 24\n",
    "    \n",
    "    for completion in completions:\n",
    "        word_count = len(completion.split())\n",
    "        \n",
    "        if word_count > max_allowed:\n",
    "            scores.append(-2.0)\n",
    "        elif word_count < 5:\n",
    "            scores.append(-2.0)\n",
    "        else:\n",
    "            # smooth penalty for being over optimal length\n",
    "            deviation = max(0, word_count - optimal_length)\n",
    "            penalty = -0.2 * deviation\n",
    "            scores.append(penalty)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def compute_coherence_penalty(joke: str, penalty_weight: float = 0.5) -> float:\n",
    "    \"\"\"Penalize incoherent jokes with rare/technical terms\"\"\"\n",
    "    rare_word_pattern = r'\\b[A-Z][a-z]*(?:-[a-z]+)*\\b'\n",
    "    rare_words = len(re.findall(rare_word_pattern, joke))\n",
    "    \n",
    "    words = joke.split()\n",
    "    if len(words) > 0:\n",
    "        rare_word_ratio = rare_words / len(words)\n",
    "        if rare_word_ratio > 0.2:  # >20% rare words = suspicious\n",
    "            return -penalty_weight * (rare_word_ratio - 0.2)\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "def coherence_penalty(completions, **kwargs):\n",
    "    return [compute_coherence_penalty(c) for c in completions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669cd716-80cc-4205-be78-01cc252a9bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "[W1123 17:31:31.996562034 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0929d8ae8324d619d754c5302271a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 53.07it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "reward_fns = [\n",
    "    roberta_score,\n",
    "    structure_diversity_reward,\n",
    "    word_pair_prompt_adherence,\n",
    "    formatting,\n",
    "    length_penalty,\n",
    "    headline_adherence,\n",
    "    coherence_penalty\n",
    "]\n",
    "reward_weights = [1.0, 1.5, 2.0, 0.5, 0.5, 2.0, 0.5]\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen2.5-1.5B-Instruct-GRPO\", \n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=3,\n",
    "    use_vllm=True,\n",
    "    vllm_mode=\"colocate\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    max_completion_length=64,\n",
    "    temperature=0.7,\n",
    "    reward_weights=reward_weights,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_id,\n",
    "    reward_funcs=reward_fns,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0214794-e9f0-4ec9-864b-13a711ae6bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkonrad-brg\u001b[0m (\u001b[33mkonrad-brg-university-of-t-bingen\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/wandb/run-20251123_173146-4hg9un5u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface/runs/4hg9un5u' target=\"_blank\">balmy-gorge-100</a></strong> to <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface/runs/4hg9un5u' target=\"_blank\">https://wandb.ai/konrad-brg-university-of-t-bingen/huggingface/runs/4hg9un5u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Initializing weave.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Failed to automatically initialize Weave: SyncClientSession.execute() takes 2 positional arguments but 3 were given\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1080/12000 08:25 < 1:25:16, 2.13 it/s, Epoch 0.27/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "/opt/conda/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2eec94-45c7-43e5-88f4-c02ec22e50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model = trainer.model\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc943973-62a8-410b-b82d-08defa4a86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Generate a funny joke that contains the words 'microwave' and 'shoes'.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=50,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aae8a9-94ab-4b04-a886-5b3daa6dda6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f3ea0-53c4-46d3-b1e6-436d151dc8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
