{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d95ebe-af93-4901-b43f-a2d16a3c4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets trl[vllm] wandb weave openai accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7b840cd-331f-4591-926b-82181cd4dae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:05 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a350cad8-d868-46d6-9c0d-12281fc6e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files=\"data/rl_df.parquet\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cffa9f5-40c2-4cdc-9cc3-b248d66c910f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Generate a funny joke related to this headline: 'Electricity costs jolt New Jersey's race for governor — and preview next year's midterms' by either modifying it or responding to it.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfcd0b0-88e9-4cf5-b578-6dda26bdf2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "scoring_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "scoring_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "def generate_response(prompt, temperature=0.6, top_p=0.9, max_new_tokens=512):\n",
    "    with torch.no_grad():\n",
    "        inputs = scoring_tokenizer(prompt, return_tensors=\"pt\").to(scoring_model.device)\n",
    "        output = scoring_model.generate(\n",
    "            **inputs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return scoring_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def construct_prompt(persona, joke):\n",
    "    return f\"\"\"\n",
    "You are a person who enjoys {persona} humour. \n",
    "Do you think the following joke is funny: \\n{joke}\\n\n",
    "Reply with a valid JSON object that contains `final_answer` (either \"yes\" or \"no\") and `reason`.\n",
    "\"\"\"\n",
    "\n",
    "def extract_json(text):\n",
    "    text = text.strip().removeprefix(\"```json\").removesuffix(\"```\")\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        # crude fallback\n",
    "        text = text[text.find(\"{\"):text.rfind(\"}\")+1]\n",
    "        return json.loads(text)\n",
    "\n",
    "def get_crowd_score(joke):\n",
    "    personas = [\"self-defeating\", \"affiliative\", \"self-enhancing\", \"aggressive\"]\n",
    "\n",
    "    def ask(persona):\n",
    "        prompt = construct_prompt(persona, joke)\n",
    "        resp = generate_response(prompt)\n",
    "        try:\n",
    "            data = extract_json(resp)\n",
    "            return 1.0 if data[\"final_answer\"].lower() == \"yes\" else 0.0\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(executor.map(ask, personas))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return sum(results)\n",
    "\n",
    "def crowd_score_rewards(completions, **kwargs):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        return list(executor.map(get_crowd_score, completions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669cd716-80cc-4205-be78-01cc252a9bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:12 [utils.py:328] non-default args: {'seed': 0, 'max_model_len': 768, 'distributed_executor_backend': 'external_launcher', 'gpu_memory_utilization': 0.3, 'max_num_batched_tokens': 4096, 'max_num_seqs': 8, 'logprobs_mode': 'processed_logprobs', 'disable_log_stats': True, 'model_impl': 'vllm', 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}\n",
      "INFO 10-27 20:35:17 [__init__.py:742] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:17 [__init__.py:1815] Using max model len 768\n",
      "INFO 10-27 20:35:17 [parallel.py:348] Disabling V1 multiprocessing for external launcher.\n",
      "INFO 10-27 20:35:18 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "INFO 10-27 20:35:18 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 10-27 20:35:20 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-27 20:35:20 [gpu_model_runner.py:2338] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1027 20:35:20.923871005 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:20 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 10-27 20:35:20 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 10-27 20:35:20 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 10-27 20:35:21 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c80cf208594ba08eb5098913cb98fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:21 [default_loader.py:268] Loading weights took 0.18 seconds\n",
      "INFO 10-27 20:35:21 [gpu_model_runner.py:2392] Model loading took 0.9286 GiB and 0.750990 seconds\n",
      "INFO 10-27 20:35:24 [backends.py:539] Using cache directory: /home/jovyan/.cache/vllm/torch_compile_cache/29155805d5/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 10-27 20:35:24 [backends.py:550] Dynamo bytecode transform time: 2.62 s\n",
      "INFO 10-27 20:35:25 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.879 s\n",
      "INFO 10-27 20:35:26 [monitor.py:34] torch.compile takes 2.62 s in total\n",
      "INFO 10-27 20:35:26 [gpu_worker.py:298] Available KV cache memory: 12.27 GiB\n",
      "INFO 10-27 20:35:26 [kv_cache_utils.py:864] GPU KV cache size: 1,071,760 tokens\n",
      "INFO 10-27 20:35:26 [kv_cache_utils.py:868] Maximum concurrency for 768 tokens per request: 1395.52x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 5/5 [00:00<00:00, 61.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:35:27 [gpu_model_runner.py:3118] Graph capturing finished in 1 secs, took 0.09 GiB\n",
      "INFO 10-27 20:35:27 [gpu_worker.py:391] Free memory on device (41.15/44.52 GiB) on startup. Desired GPU memory utilization is (0.3, 13.36 GiB). Actual usage is 0.93 GiB for weight, 0.15 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.09 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=12920272896` to fit into requested memory, or `--kv-cache-memory=42762254336` to fully utilize gpu memory. Current kv cache memory in use is 13169833984 bytes.\n",
      "INFO 10-27 20:35:27 [core.py:218] init engine (profile, create kv cache, warmup model) took 5.85 seconds\n",
      "INFO 10-27 20:35:28 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 10-27 20:35:28 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    }
   ],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=\"Qwen/Qwen2.5-0.5B-Instruct-GRPO\", \n",
    "    report_to=\"wandb\",\n",
    "    num_train_epochs=1,\n",
    "    use_vllm=True,\n",
    "    vllm_mode=\"colocate\",\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    reward_funcs=crowd_score_rewards,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0214794-e9f0-4ec9-864b-13a711ae6bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/1200 02:35 < 10:17:16, 0.03 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-27 20:36:19 [block_pool.py:292] Successfully reset prefix cache\n",
      "INFO 10-27 20:36:55 [block_pool.py:292] Successfully reset prefix cache\n",
      "INFO 10-27 20:37:25 [block_pool.py:292] Successfully reset prefix cache\n",
      "INFO 10-27 20:37:57 [block_pool.py:292] Successfully reset prefix cache\n",
      "INFO 10-27 20:38:27 [block_pool.py:292] Successfully reset prefix cache\n",
      "INFO 10-27 20:38:54 [block_pool.py:292] Successfully reset prefix cache\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894ad1a-f482-4569-9d53-47e551a07369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b871f-b4a6-4aeb-90bb-ce44acb06328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trainer.processing_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2eec94-45c7-43e5-88f4-c02ec22e50c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
