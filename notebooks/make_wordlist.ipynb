{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d01b72e-ed7b-426c-b762-582a6e80371b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (Mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission</td>\n",
       "      <td>ticket</td>\n",
       "      <td>5.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>4.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>6.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "      <td>2.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>news</td>\n",
       "      <td>7.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 1     Word 2  Human (Mean)\n",
       "0     admission     ticket        5.5360\n",
       "1       alcohol  chemistry        4.1250\n",
       "2      aluminum      metal        6.6250\n",
       "3  announcement     effort        2.0625\n",
       "4  announcement       news        7.1875"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wl = pd.read_csv(\"wordsim353crowd.csv\")\n",
    "wl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c350e9a0-2cba-4cf0-9056-369b403c793e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "353"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wl.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06b78f12-bc64-4e48-a434-6ead04309edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>announcement</td>\n",
       "      <td>production</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arafat</td>\n",
       "      <td>Jackson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arafat</td>\n",
       "      <td>peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arafat</td>\n",
       "      <td>terror</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 1      Word 2\n",
       "0  announcement      effort\n",
       "1  announcement  production\n",
       "2        Arafat     Jackson\n",
       "3        Arafat       peace\n",
       "4        Arafat      terror"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl_low_sim = wl[wl[\"Human (Mean)\"] <= 4].copy().reset_index(drop=True)[[\"Word 1\", \"Word 2\"]]\n",
    "wl_low_sim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea889347-9bf6-43a3-8b9f-f8c98d8554f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_low_sim.to_csv(\"custom_word_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e004cf4-1cc8-4fe2-99c0-6b00a9bb6fef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bec6e5e31c4a789337b7f0676389c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd54cfc-5eaa-48e8-8fd8-c3f4b53aefe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a634dc46f9c548d0994135a7b1283f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96576f6cad2b44cdb1ac08b4690a8ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9daba0be4d842f4aed21f93bb5704c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588d5bc2816f4b81aa02cf0872e3a8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b225e2c05c4b57bc8c8dff3a6f6455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6ec464ce5f44c0ac86716acaee0cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a718119669b144d8b7819cc5154a429e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f781cc53ecc43a99fe0dd08ee2edf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f540103a41a457299c4633a29491b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c438bfccf69342baa02ffc52f0745569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79da9c1b3dd417591d3a8cdeb436e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a30e7695c241de9e89071ee0e2f16e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6220b09c-e8fd-4aba-a48c-ba31ff9be5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a funny joke that contains the following two words: `announcement` and `effort`. Return only the joke. \\n\\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    f\"Generate a funny joke that contains the following two words: `{w1}` and `{w2}`. Return only the joke. \\n\\n\"\n",
    "    for (w1, w2) in zip(wl_low_sim[\"Word 1\"], wl_low_sim[\"Word 2\"])\n",
    "]\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cb194a1f-0bd8-4f64-b6b6-380c61d996d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Get a list of common nouns\n",
    "nouns = [w.name().split('.')[0] for w in wn.all_synsets('n')]\n",
    "nouns = list(set(nouns))  # remove duplicates\n",
    "\n",
    "def wordnet_similarity(w1, w2):\n",
    "    syns1 = wn.synsets(w1)\n",
    "    syns2 = wn.synsets(w2)\n",
    "    if not syns1 or not syns2:\n",
    "        return 0\n",
    "    # Take max path similarity among all combinations\n",
    "    sims = [s1.path_similarity(s2) for s1 in syns1 for s2 in syns2 if s1.path_similarity(s2)]\n",
    "    return max(sims) if sims else 0\n",
    "\n",
    "def random_low_similarity_pairs(n=10, threshold=0.2):\n",
    "    pairs = []\n",
    "    while len(pairs) < n:\n",
    "        w1, w2 = random.sample(nouns, 2)\n",
    "        sim = wordnet_similarity(w1, w2)\n",
    "        if sim < threshold:\n",
    "            w1 = w1.replace(\"_\", \" \")\n",
    "            w2 = w2.replace(\"_\", \" \")\n",
    "            pairs.append((w1, w2))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fa6e8eb6-0ae2-4648-b574-0f754295a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = random_low_similarity_pairs(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "96c10d5d-a06b-48c0-a4cf-147bc8dad239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Generate a funny joke that contains the following two words: `silphium` and `snakeblenny`. Return only the joke. \\n\\n'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = [\n",
    "    f\"Generate a funny joke that contains the following two words: `{w1}` and `{w2}`. Return only the joke. \\n\\n\"\n",
    "    for (w1, w2) in pairs\n",
    "]\n",
    "prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d3174001-4efa-4937-9515-b3f6a8f86751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class ListDataset(Dataset):\n",
    "    def __init__(self, original_list):\n",
    "        self.original_list = original_list\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_list)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        messages = [{\"role\": \"user\", \"content\": self.original_list[i]}]\n",
    "        return self.tokenizer.apply_chat_template(messages, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7edd45dd-7e89-4c4d-864f-83072f69c243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [1:21:27<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_dataset = ListDataset(prompts)\n",
    "generated_jokes = []\n",
    "for out in tqdm(pipe(prompt_dataset, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    generated_jokes.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "df9e6e52-4057-4c84-a93c-f1b4cf284d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the Bairdiella fish go to the party with the ocean expert? Because it wanted to meet a bigger knower of the sea.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_jokes[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "96f1cdf6-cae9-47bd-bb32-932976cd1adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_prompt = \"\"\"\n",
    "You are a person who enjoys observational humour. \n",
    "Observational jokes are an examination of everyday things or situations through a comedic lens. \n",
    "Observational comedy covers topics familiar to almost everyone, even the most trivial aspects of life.\n",
    "Do you think the following joke is funny or boring? {}\n",
    "Reply either `It is funny.` or `It is boring.` followed by a brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "1f99815b-d6c5-46a0-a6eb-b73d6c9e5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "anec_prompt = \"\"\"\n",
    "You are a person who enjoys anecdotal humour.\n",
    "Anecdotal humor is pulled from the comedian’s personal life and is popular with audiences because we can identify with their stories. \n",
    "Writer, producer and director Judd Apatow, who also performs stand-up comedy, believes that stand-up gets better as it becomes more personal—that comics who lay themselves bare to the audience are often the strongest performers. \n",
    "He gives the following example: one of his daughters has gone to college. \n",
    "His remaining daughter is unhappy that she is the only one left in the house with Judd and his wife, because four people is a family, but three people is a child observing a weird couple. \n",
    "You get the most laughs when the audience recognizes themselves in your story or joke.\n",
    "Do you think the following joke is funny or boring? {}\n",
    "Reply either `It is funny.` or `It is boring.` followed by a brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "714cd985-38dc-42ef-923b-95d297e1badc",
   "metadata": {},
   "outputs": [],
   "source": [
    "onel_prompt = \"\"\"\n",
    "You are a person who enjoys One-liners. \n",
    "“I’ve had a perfectly wonderful evening, but this wasn’t it.”\n",
    "That one-liner was delivered by Groucho Marx. \n",
    "Robin Williams once joked, “Why do they call it rush hour when nothing moves?” \n",
    "One-liners squeeze a setup and a punchline into one succinct thought.\n",
    "Do you think the following joke is funny or boring? {}\n",
    "Reply either `It is funny.` or `It is boring.` followed by a brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aa3653bb-6d78-4052-b83e-06e4f5273dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "iro_prompt = \"\"\"\n",
    "You are a person who enjoys Ironic jokes. \n",
    "Ironic jokes are contradictory, with two opposing concepts tugging at one another. \n",
    "For example, why do people park in a driveway but drive on a parkway?\n",
    "Do you think the following joke is funny or boring? {}\n",
    "Reply either `It is funny.` or `It is boring.` followed by a brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f1ac0427-6a61-49e6-a7d3-be14aca6289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_def_prompt = \"\"\"\n",
    "You are a person who enjoys Self-deprecating humour. \n",
    "Some comedians make fun of the person they know best—themselves. \n",
    "Rodney Dangerfield made a career of self-deprecating jokes, poking fun at his looks and his love life with jokes like this: \n",
    "“I went to the psychiatrist, and he says ‘You're crazy.’ \n",
    "I tell him I want a second opinion. \n",
    "He says, ‘Okay, you're ugly too!’”\n",
    "Do you think the following joke is funny or boring? {}\n",
    "Reply either `It is funny.` or `It is boring.` followed by a brief justification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "165d3885-d1ef-4d4c-9df5-042da896972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.1:28:11<42:10,  1.57s/it]  \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obs_prompts = [obs_prompt.format(jk) for jk in generated_jokes]\n",
    "obs_prompts = ListDataset(obs_prompts)\n",
    "obs_evals = []\n",
    "for out in tqdm(pipe(obs_prompts, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    obs_evals.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f35025b7-42c8-4cec-b714-3bdfaa8a2dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the silphium farmer's pet snakeblenny go to therapy? Because it had a hiss-terical problem with its roots.\n",
      "\n",
      "Eval:\n",
      "It is funny. \n",
      "\n",
      "This joke is funny because it's an unexpected twist on a common concept (punching cards) and applies it to a specific situation (the Hollerith card) and then takes it to an absurd level by introducing the idea of a \"jumping orchid\" which\n"
     ]
    }
   ],
   "source": [
    "print(generated_jokes[0])\n",
    "print(\"\\nEval:\")\n",
    "print(obs_evals[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "159cfabf-dc47-4ac2-ac0c-4ff566c1aa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.1:18:21<52:33,  1.58s/it]  \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 5000/5000 [2:10:36<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "anec_prompts = [anec_prompt.format(jk) for jk in generated_jokes]\n",
    "anec_prompts = ListDataset(anec_prompts)\n",
    "anec_evals = []\n",
    "for out in tqdm(pipe(anec_prompts, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    anec_evals.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2c1e0024-e688-4fc2-9eb0-716f7a337d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is boring. \\n\\nThis joke relies on a play on words with \"knower of the sea\" and \"know a bigger sea,\" but it\\'s a one-liner that lacks personal connection or relatability. It doesn\\'t draw from the comedian\\'s own life or experiences, which makes it'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anec_evals[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2ddea5fa-88de-43f7-9278-d23dc5c05785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.1:14:59<50:28,  1.51s/it]  \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "onel_prompts = [onel_prompt.format(jk) for jk in generated_jokes]\n",
    "onel_prompts = ListDataset(onel_prompts)\n",
    "onel_evals = []\n",
    "for out in tqdm(pipe(onel_prompts, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    onel_evals.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ac179297-b954-4d66-a687-14b4ee09f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.1:17:35<45:05,  1.35s/it]  \n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iro_prompts = [iro_prompt.format(jk) for jk in generated_jokes]\n",
    "iro_prompts = ListDataset(iro_prompts)\n",
    "iro_evals = []\n",
    "for out in tqdm(pipe(iro_prompts, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    iro_evals.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "61d83cad-8b65-418a-ba4d-f0df4c3f6117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [2:08:33<00:00,  1.54s/it]  \n"
     ]
    }
   ],
   "source": [
    "sd_prompts = [self_def_prompt.format(jk) for jk in generated_jokes]\n",
    "sd_prompts = ListDataset(sd_prompts)\n",
    "sf_evals = []\n",
    "for out in tqdm(pipe(sd_prompts, pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=64)):\n",
    "    response = out[0][\"generated_text\"].split(\"<|eot_id|>assistant\\n\\n\")[1]\n",
    "    sf_evals.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e1cca267-bbda-4fd0-ab22-a2a04583b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame()\n",
    "eval_df[\"joke\"] = generated_jokes\n",
    "eval_df[\"anecdotal\"] = [s.split()[2].rstrip('.') for s in anec_evals]\n",
    "eval_df[\"observational\"] = [s.split()[2].rstrip('.') for s in obs_evals]\n",
    "eval_df[\"one_liner\"] = [s.split()[2].rstrip('.') for s in onel_evals]\n",
    "eval_df[\"irony\"] = [s.split()[2].rstrip('.') for s in iro_evals]\n",
    "eval_df[\"self_deprecating\"] = [s.split()[2].rstrip('.') for s in sf_evals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6740296b-4cfe-4810-a5f5-10146a37bbd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "      <th>anecdotal</th>\n",
       "      <th>observational</th>\n",
       "      <th>one_liner</th>\n",
       "      <th>irony</th>\n",
       "      <th>self_deprecating</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why did the silphium farmer's pet snakeblenny ...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did the basal body temperature go to thera...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why did the hollerith card go to therapy after...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>boring</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why did the Tocharian singer break up with his...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why did the cormorant, a member of the Phalacr...</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                joke anecdotal observational  \\\n",
       "0  Why did the silphium farmer's pet snakeblenny ...    boring         funny   \n",
       "1  Why did the basal body temperature go to thera...    boring         funny   \n",
       "2  Why did the hollerith card go to therapy after...    boring         funny   \n",
       "3  Why did the Tocharian singer break up with his...    boring         funny   \n",
       "4  Why did the cormorant, a member of the Phalacr...     funny         funny   \n",
       "\n",
       "  one_liner  irony self_deprecating  score  \n",
       "0     funny  funny            funny      4  \n",
       "1     funny  funny            funny      4  \n",
       "2     funny  funny           boring      3  \n",
       "3     funny  funny            funny      4  \n",
       "4     funny  funny            funny      5  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[\"score\"] = eval_df.apply(\n",
    "    lambda row: sum(str(cell).lower().count('funny') for cell in row[1:]),\n",
    "    axis=1\n",
    ")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9e315993-68dc-4f87-9041-ce031767114d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "      <th>anecdotal</th>\n",
       "      <th>observational</th>\n",
       "      <th>one_liner</th>\n",
       "      <th>irony</th>\n",
       "      <th>self_deprecating</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why did the silphium farmer's pet snakeblenny ...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did the basal body temperature go to thera...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why did the hollerith card go to therapy after...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>boring</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why did the Tocharian singer break up with his...</td>\n",
       "      <td>boring</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why did the cormorant, a member of the Phalacr...</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                joke anecdotal observational  \\\n",
       "0  Why did the silphium farmer's pet snakeblenny ...    boring         funny   \n",
       "1  Why did the basal body temperature go to thera...    boring         funny   \n",
       "2  Why did the hollerith card go to therapy after...    boring         funny   \n",
       "3  Why did the Tocharian singer break up with his...    boring         funny   \n",
       "4  Why did the cormorant, a member of the Phalacr...     funny         funny   \n",
       "\n",
       "  one_liner  irony self_deprecating  score  \n",
       "0     funny  funny            funny      4  \n",
       "1     funny  funny            funny      4  \n",
       "2     funny  funny           boring      3  \n",
       "3     funny  funny            funny      4  \n",
       "4     funny  funny            funny      5  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "96aa85b8-8fe1-4a2d-9f62-7e5471366e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df.to_csv(\"eval_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3ee29afc-e3f9-4ee5-aa3a-819e87189b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, eval_df):\n",
    "        self.eval_df = eval_df.copy()\n",
    "        \n",
    "        # get the 5 individual joke category evals\n",
    "        self.label_columns = ['anecdotal', 'observational', 'one_liner', 'irony', 'self_deprecating']\n",
    "        self.eval_df[self.label_columns] = self.eval_df[self.label_columns].replace({'funny': 1, 'boring': 0})\n",
    "        self.eval_df = self.eval_df[self.eval_df[self.label_columns].apply(lambda x: x.isin([0, 1]).all(), axis=1)]\n",
    "        \n",
    "        self.labels = self.eval_df[self.label_columns].to_dict(orient='records')\n",
    "        self.jokes = self.eval_df.joke.to_list()\n",
    "        \n",
    "        # load tokenizer and model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "        self.model = AutoModel.from_pretrained(\"FacebookAI/xlm-roberta-large\").to(\"cuda\")\n",
    "        self.model.eval()\n",
    "        \n",
    "        # generate embeddings\n",
    "        self.features = self._featurize_all()\n",
    "    \n",
    "    def _featurize_all(self):\n",
    "        features = []\n",
    "        device = next(self.model.parameters()).device\n",
    "        \n",
    "        for text in tqdm(self.jokes):\n",
    "            with torch.no_grad():\n",
    "                tokens = self.tokenizer(text, return_tensors=\"pt\", truncation=True, \n",
    "                                       padding=True, max_length=128).to(device)\n",
    "                outputs = self.model(**tokens)\n",
    "                cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze(0)\n",
    "                features.append(cls_embedding.cpu())\n",
    "        \n",
    "        return torch.stack(features)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        features = self.features[i]\n",
    "        labels_dict = {\n",
    "            'anecdotal': torch.tensor(self.labels[i].get('anecdotal', 0), dtype=torch.long),\n",
    "            'observational': torch.tensor(self.labels[i].get('observational', 0), dtype=torch.long),\n",
    "            'ironic': torch.tensor(self.labels[i].get('irony', 0), dtype=torch.long),\n",
    "            'one_liner': torch.tensor(self.labels[i].get('one_liner', 0), dtype=torch.long),\n",
    "            'self_deprecating': torch.tensor(self.labels[i].get('self_deprecating', 0), dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        return {\"features\": features, \"labels\": labels_dict}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "88e6fbb0-c54a-4537-be5f-725917d7637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class JokeEvaluationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_labels=5, embedding_dim=768):\n",
    "        super().__init__()\n",
    "        self.shared_fc = nn.Linear(embedding_dim, 512)\n",
    "        self.drop = nn.Dropout(0.3)\n",
    "\n",
    "        self.anec_clf = nn.Linear(512, 2)\n",
    "        self.obs_clf = nn.Linear(512, 2)\n",
    "        self.iro_clf = nn.Linear(512, 2)\n",
    "        self.onel_clf = nn.Linear(512, 2)\n",
    "        self.sd_clf = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, features, labels, **kwargs):\n",
    "\n",
    "        x = self.drop(features)\n",
    "        x = self.shared_fc(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x_a = self.anec_clf(x)\n",
    "        x_ob = self.obs_clf(x)\n",
    "        x_iro = self.iro_clf(x)\n",
    "        x_on = self.onel_clf(x)\n",
    "        x_sd = self.sd_clf(x)\n",
    "\n",
    "        return x_a, x_ob, x_iro, x_on, x_sd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a3f225fb-c08c-4502-a75b-850bc8e9e34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68/1844628415.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  self.eval_df[self.label_columns] = self.eval_df[self.label_columns].replace({'funny': 1, 'boring': 0})\n",
      "100%|██████████| 3494/3494 [00:19<00:00, 177.66it/s]\n",
      "/tmp/ipykernel_68/1844628415.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  self.eval_df[self.label_columns] = self.eval_df[self.label_columns].replace({'funny': 1, 'boring': 0})\n",
      "100%|██████████| 750/750 [00:04<00:00, 177.05it/s]\n",
      "/tmp/ipykernel_68/1844628415.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  self.eval_df[self.label_columns] = self.eval_df[self.label_columns].replace({'funny': 1, 'boring': 0})\n",
      "100%|██████████| 747/747 [00:04<00:00, 176.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train (70%) and temp (30%)\n",
    "train_df, temp_df = train_test_split(eval_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp into val (15%) and test (15%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = EvalDataset(train_df)\n",
    "val_dataset = EvalDataset(val_df)\n",
    "test_dataset = EvalDataset(test_df)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "84edd585-477b-4059-a4ce-b1f063fc89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        features = batch[\"features\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "        \n",
    "        # move all labels to device\n",
    "        anec_labels = labels[\"anecdotal\"].to(device)\n",
    "        obs_labels = labels[\"observational\"].to(device)\n",
    "        iro_labels = labels[\"ironic\"].to(device)\n",
    "        onel_labels = labels[\"one_liner\"].to(device)\n",
    "        sd_labels = labels[\"self_deprecating\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        x_a, x_ob, x_iro, x_on, x_sd = model(features=features, labels=labels)\n",
    "        \n",
    "        # compute losses for each classifier\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss_a = criterion(x_a, anec_labels)\n",
    "        loss_ob = criterion(x_ob, obs_labels)\n",
    "        loss_iro = criterion(x_iro, iro_labels)\n",
    "        loss_on = criterion(x_on, onel_labels)\n",
    "        loss_sd = criterion(x_sd, sd_labels)\n",
    "        \n",
    "        # total loss -> sum of all classifier losses\n",
    "        total_batch_loss = loss_a + loss_ob + loss_iro + loss_on + loss_sd\n",
    "        \n",
    "        # backward pass\n",
    "        total_batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += total_batch_loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader):\n",
    "            features = batch[\"features\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "            \n",
    "            anec_labels = labels[\"anecdotal\"].to(device)\n",
    "            obs_labels = labels[\"observational\"].to(device)\n",
    "            iro_labels = labels[\"ironic\"].to(device)\n",
    "            onel_labels = labels[\"one_liner\"].to(device)\n",
    "            sd_labels = labels[\"self_deprecating\"].to(device)\n",
    "            \n",
    "            x_a, x_ob, x_iro, x_on, x_sd = model(features=features, labels=labels)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss_a = criterion(x_a, anec_labels)\n",
    "            loss_ob = criterion(x_ob, obs_labels)\n",
    "            loss_iro = criterion(x_iro, iro_labels)\n",
    "            loss_on = criterion(x_on, onel_labels)\n",
    "            loss_sd = criterion(x_sd, sd_labels)\n",
    "            \n",
    "            total_batch_loss = loss_a + loss_ob + loss_iro + loss_on + loss_sd\n",
    "            total_loss += total_batch_loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs=10, learning_rate=1e-4, device='cuda'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    print(f\"\\nTraining complete. Best validation loss: {best_val_loss:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9a55e893-ebe2-4616-8e5f-4b384e7c32ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 561.38it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1137.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  Train Loss: 1.5053\n",
      "  Val Loss: 1.1763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 557.53it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1144.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n",
      "  Train Loss: 1.1476\n",
      "  Val Loss: 1.1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 560.48it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1163.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n",
      "  Train Loss: 1.1370\n",
      "  Val Loss: 1.1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 576.01it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1149.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n",
      "  Train Loss: 1.1248\n",
      "  Val Loss: 1.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 578.11it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1141.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n",
      "  Train Loss: 1.1093\n",
      "  Val Loss: 1.1854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 581.28it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1141.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "  Train Loss: 1.1006\n",
      "  Val Loss: 1.1484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 582.69it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1154.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n",
      "  Train Loss: 1.1075\n",
      "  Val Loss: 1.1519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 584.53it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1150.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n",
      "  Train Loss: 1.0899\n",
      "  Val Loss: 1.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 583.79it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1152.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "  Train Loss: 1.0828\n",
      "  Val Loss: 1.1733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 584.68it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1164.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n",
      "  Train Loss: 1.0853\n",
      "  Val Loss: 1.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 578.25it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1160.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "  Train Loss: 1.0861\n",
      "  Val Loss: 1.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 584.47it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1162.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n",
      "  Train Loss: 1.0741\n",
      "  Val Loss: 1.1457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 586.85it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1166.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n",
      "  Train Loss: 1.0686\n",
      "  Val Loss: 1.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 584.53it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1164.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "  Train Loss: 1.0724\n",
      "  Val Loss: 1.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 586.04it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1154.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n",
      "  Train Loss: 1.0721\n",
      "  Val Loss: 1.1070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 585.38it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1172.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "  Train Loss: 1.0666\n",
      "  Val Loss: 1.1150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 586.00it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1164.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "  Train Loss: 1.0508\n",
      "  Val Loss: 1.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 586.39it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1163.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "  Train Loss: 1.0646\n",
      "  Val Loss: 1.1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 585.05it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1156.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "  Train Loss: 1.0596\n",
      "  Val Loss: 1.1609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:00<00:00, 585.18it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 1158.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "  Train Loss: 1.0564\n",
      "  Val Loss: 1.1084\n",
      "\n",
      "Training complete. Best validation loss: 1.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JokeEvaluationModel(\n",
       "  (shared_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (drop): Dropout(p=0.3, inplace=False)\n",
       "  (anec_clf): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (obs_clf): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (iro_clf): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (onel_clf): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (sd_clf): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = JokeEvaluationModel(embedding_dim=1024).to(\"cuda\")\n",
    "train(model, train_loader, val_loader, epochs=20, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd268f-98ed-468e-b1e6-e729e8b80b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
